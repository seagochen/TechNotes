@[toc]


# Authors and Publishment

## Authors
Karen Simonyan / Visual Geometry Group, Department of Engineering Science, University of Oxford
Andrew Zisserman / Visual Geometry Group, Department of Engineering Science, University of Oxford

## Bibtex

Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.


# Abstract
In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. 

> 在这项工作中，我们研究了卷积网络深度对其在大规模图像识别设置中的准确性的影响。 我们主要的贡献在于使用非常小的（3 x 3）的卷积滤波，通过增加深度对网络进行了全面的评估，并表明通过把网络深度增加到16-19层后，可以对现有的技术精度进行明显的改善。

These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.

> 这些发现是我们提交 2014 年 ImageNet 挑战赛的基础，我们的团队分别获得了定位和分类赛道的第一和第二名。我们还发现，我们的方法可以很好地推广到其他数据集，并在这些数据集上取得了最先进的结果。我们已经公开了两个性能最佳的卷积网络（ConvNet）模型，以促进进一步研究在计算机视觉中使用深度视觉表示。


# 1. Introduction

Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale im- age and video recognition (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014; Simonyan & Zisserman, 2014) which has become possible due to the large public image repositories, such as ImageNet (Deng et al., 2009), and high-performance computing systems, such as GPUs or large-scale distributed clusters (Dean et al., 2012). 

> 卷积网络 (ConvNets) 最近在大规模图像和视频识别方面取得了巨大成功 (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014; Simonyan & Zisserman, 2014) 由于大型公共图像存储库，如 ImageNet (Deng et al., 2009) 和高性能计算系统，如 GPU 或大规模分布式集群 (Dean et al., 2012)，使得应用成为可能。

In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2014), which has served as a testbed for a few generations of large-scale image classification systems, from high-dimensional shallow feature encodings (Perronnin et al., 2010) (the winner of ILSVRC-2011) to deep ConvNets (Krizhevsky et al., 2012) (the winner of ILSVRC-2012).

> 特别是，ImageNet 大规模视觉识别挑战赛 (ILSVRC) (Russakovsky et al., 2014) 在深度视觉识别架构的发展中发挥了重要作用，并作为数代图像分类系统使用的测试平台，从高维浅层特征编码 (Perronnin et al., 2010) (ILSVRC-2011 的获胜者) 到深度卷积网络 (Krizhevsky et al., 2012) (ILSVRC-的获胜者) 。

With ConvNets becoming more of a commodity in the computer vision field, a number of attempts have been made to improve the original architecture of Krizhevsky et al. (2012) in a bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC- 2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer. 

> 随着卷积网络在计算机视觉领域成为一种工具，基于 Krizhevsky 等人（2012）的原始架构做了很多改进，以达到更好的准确性。 例如，在ILSVRC-2013中表现最好的（Zeiler & Fergus, 2013; Sermanet et al., 2014）使用了更小的窗口和在其第一个层卷积层中使用了更小的步幅。

Another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales (Sermanet et al., 2014; Howard, 2014). In this paper, we address another important aspect of ConvNet architecture design – its depth. To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 × 3) convolution filters in all layers.

> 另一项改进涉及在整个图像和多个尺度上密集地训练和测试网络（Sermanet 等人，2014 年；霍华德，2014 年）。在本文中，我们讨论了卷积网络架构设计的另一个重要方面——深度。 为此，我们固化了除此之外的其他参数，并通逐步在网络中增加卷积层数，并且所有层中都使用了非常小的（3 x 3）卷积滤波器。

As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning). We have released our two best-performing models1 to facilitate further research.

> 因此，我们得到了更精确的卷积网络架构，它不仅在 ILSVRC 分类和定位任务上达到了最先进的精度，而且还适用于其他图像识别数据集，即便我们仅使用相对简单的一部分管道时（例如，由线性 SVM 分类的深层特征，无需微调）。我们发布了两个性能最佳的模型[^1]，以促进未来的研究。

[^1]:  http://www.robots.ox.ac.uk/ ̃vgg/research/very_deep/

The rest of the paper is organised as follows. In Sect. 2, we describe our ConvNet configurations. The details of the image classification training and evaluation are then presented in Sect. 3, and the configurations are compared on the ILSVRC classification task in Sect. 4. Sect. 5 concludes the paper. 

> 本文的其余部分安排如下。 在章节2中，我们描述了我们的卷积网络的配置情况。关于图像分类训练和评估的细节将放在第 3 节中介绍，对于 ILSVRC 上对配置的比较则在章节4中，章节 5 则是论文的总结。

For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B. Finally, Appendix C contains the list of major paper revisions.

> 为了完整起见，我们还在附录 A 中描述和评估了我们在 ILSVRC-2014 中的定位系统，以及在附录B种讨论对于不同数据集的泛化能力。以及在附录C中，包含论文的修订列表。


# 2. Convnet Configurations

To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by Ciresan et al. (2011); Krizhevsky et al. (2012). In this section, we first describe a generic layout of our ConvNet configurations (Sect. 2.1) and then detail the specific configurations used in the evaluation (Sect. 2.2). Our design choices are then discussed and compared to the prior art in Sect. 2.3.

> 为了公平地评估由于卷积层深度的增加带来的性能改进，受Ciresan（2011）、Krizhevsky等人（2012）等人的启发，我们所有的卷积网络配置都遵循相同的设计原则。在这个章节里，我们首先描述一个通用卷机网络结构(Sect. 2.1) ，然后再细致地介绍评估用到的结构 (Sect. 2.2)。以及在章节2.3中，我们的设计选择以及对现有技术进行比较。

## 2.1. Architecture

During training, the input to our ConvNets is a fixed-size 224 × 224 RGB image. The only preprocessing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very small receptive field: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down, center). 

> 在训练过程中，输入到我们的卷积网络图像大小是固定的224 × 224 RGB图像。唯一的预处理是对像素减去在训练集中得到的RGB均值。图像经过卷积层栈，我们使用非常小的结构 3 x 3（该结构可以捕捉到左/右，上/下，中心中最小的特征）


In one of the configurations we also utilise 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 × 3 conv. layers. 

> 在其中一处地方，我们也用到了1 x 1的卷积滤波器，可以被看作是对输入通道的线性转换（随后是非线形的）。卷积运算步进为1个像素；对于卷积的空间填充是根据卷积计算后的空间大小决定，例如，对于 3 x 3卷积，填充尺度为1个像素。

Spatial pooling is carried out by five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2 × 2 pixel window, with stride 2.

> 空间池化由五个最大池化层执行，（它们）跟随在一些卷积层后（不是所有的最大池化层都跟随着卷积层）。最大池化执行一个 2 x 2 的运算窗口，执行步进为2.


A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000-way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks.

> 卷积层栈（在不同结构有不同的深度）后跟随的是三个全连接层（FC）；前两个（全连接层）拥有4096个通道，最后一个负责执行1000路分类输出（基于ILSVRC 数据集有1000个分类）。最后一层是 soft-max 层。所有的全连接层在所有结构中都拥有相同大小。


All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al., 2012)) non-linearity. We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al., 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time. Where applicable, the parameters for the LRN layer are those of (Krizhevsky et al., 2012).

> 所有的隐藏层都使用了 ReLu  (Krizhevsky et al., 2012) 非线形输出。我们注意到我们全部的（除了一个）都不包含局部响应归一化（LRN）(Krizhevsky et al., 2012)；如同第4节所示，该归一化对于ILSVRC数据集不提高任何性能，而仅导致内存开销增加和时间消耗。在某些情况需要使用时，LRN的参数配置如(Krizhevsky et al., 2012)论文中描述一致。


## 2.2. Configurations

The ConvNet configurations, evaluated in this paper, are outlined in Table 1, one per column.  In the following we will refer to the nets by their names (A–E). All configurations follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512.

> 关于卷积网络的配置信息，在表1中进行了描述。在接下来的章节，我们使用（A- E）表示不同的网络。所有的配置遵循章节2.1中描述的通用标准，差别仅在于深度；从11级网络权重A（8个Conv.和3个FC），到19级网络权重E（16个Conv. 和3个FC）。卷积层大小都很小（通道数），从第一层的64个通道，经过每次的max-pooling层后增加2倍，直到512个通道。


In Table 2 we report the number of parameters for each configuration. In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in (Sermanet et al., 2014)).

> 在表 2 中，我们介绍了每种配置的参数情况。 随着网络深度的增加，网络权重数并不比浅层网络同时拥有大尺寸卷积的多(Sermanet et al., 2014)。


## 2.3. Discussion

Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al., 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al., 2014). 

> 我们的卷积网络配置与 ILSVRC-2012 (Krizhevsky et al., 2012) 和 ILSVRC-2013 竞赛 (Zeiler & Fergus, 2013; Sermanet et al., 2014) 中表现最好的参赛作品中使用的配置完全不同。

Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al., 2012), or 7 × 7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al., 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two 3 × 3 conv. layers (without spatial pooling in between) has an effective receptive field of 5 × 5; 

> 相较于（Krizhevsky 等人，2012）在第一个感知卷积层使用的 11 x 11，4步长，或者(Zeiler & Fergus, 2013; Sermanet et al., 2014)中使用 7 x 7，2步长。我们通篇使用了 3 x 3的感知卷积层，它和所有的像素执行卷积过程（步长1）。这很容发现两个 3 x 3卷积层执行效果与1个 5 x 5的卷积层是一样的。

>![在这里插入图片描述](https://img-blog.csdnimg.cn/135911a4a48c4a39b0c0d59ca5785745.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)
> Table 1: ConvNet configurations (shown in columns). The depth of the configurations increases from the left (A) to the right (E), as more layers are added (the added layers are shown in bold). The convolutional layer parameters are denoted as “conv⟨receptive field size⟩-⟨number of channels⟩”. The ReLU activation function is not shown for brevity.
> 表1: 卷积网络的配置（显示在列中）。 随着添加更多层（添加的层以粗体显示），配置的深度从左侧 (A) 到右侧 (E) 增加。 卷积层参数表示为“conv（感知器大小）-（通道数）”。 为简洁起见，未显示 ReLU 激活函数。

>![在这里插入图片描述](https://img-blog.csdnimg.cn/b66b74812a2c40f7b8958f3f360532ac.png#pic_center)
> Table 2: Number of parameters (in millions).
> 表2: 网络参数（百万级）


such layers have a 7 × 7 effective receptive field. So what have we gained by using, for instance, a stack of three 3 × 3 conv. layers instead of a single 7 × 7 layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 × 3 convolution stack has C channels, the stack is parametrised by $3(3^2 C^2) = 27 C^2$ weights；at the same time, a single 7 × 7 conv. layer would require $7^2 C^2 = 49 C^2$ parameters, i.e. 81% more. 

> (那些论文中)使用了 7 x 7的感知卷积层 。那么我们获得了什么，举例来说，一组3 x 3卷积层栈替代了7 x 7卷机层？首先，我们使用了三个非线性校正层而不是单个，这使得决策函数更具判别性。其次，我们减少了参数数量：假设 3 x 3卷机层的输入和输出都有C个通道，就可以计算出栈的权重大小为 $3(3^2 C^2) = 27 C^2$；同时，单个 7 x 7的卷积则需要$7^2 C^2 = 49 C^2$ 个参数，即多 81%

This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with non-linearity injected in between). The incorporation of 1 × 1 conv. layers (configuration C, Table 1) is a way to increase the non-linearity of the decision function without affecting the receptive fields of the conv. layers. 

>   这可以看作是对 7 × 7  的正则化，使得它们按照 3 × 3 卷机大小进行分解（在其间注入非线性）。加入 1 x 1 卷积层（配置C，表1）也是一种在不影响决策过程的前提下，增加了非线形。

Even though in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the rectification function. It should be noted that 1 × 1 conv. layers have recently been utilised in the “Network in Network” architecture of Lin et al. (2014).


> 尽管在我们的例子中，1 x 1 卷积实质是在相同空间中做了一次映射（输入和输出通道数相同），由矫正函数引入了额外的非线性。需要注意的是，1 x 1 卷积层被Lin等人（2014）引入到“网络中的网络”架构中。


Small-size convolution filters have been previously used by Ciresan et al. (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. Goodfellow et al. (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance. 

> 小尺寸的卷积层在Ciresan等人（2011）的工作中也曾被使用过，但他们的网络深度明显没有我们这么多，并且他们也没比较过ILSVRC数据集上的表现。Goodfellow等人（2014）使用过卷积网络（11层权重）用于街道号码识别，并且发现增加深度可以得到更好的性能。


GoogLeNet (Szegedy et al., 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets (22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions). 

>  GoogLeNet (Szegedy et al., 2014) 是 ILSVRC-2014 分类任务中表现最好的，独立于我们的工作，但相似之处在于它基于非常深的卷积层（22 个权重层）和小卷积过滤器（除了 3 × 3，它们还使用 1 × 1 和 5 × 5 卷积）。

Their network topology is, however, more complex than ours, and the spatial resolution of the feature maps is reduced more aggressively in the first layers to decrease the amount of computation. As will be shown in Sect. 4.5, our model is outperforming that of Szegedy et al. (2014) in terms of the single-network classification accuracy.

> 显然，它们的网络拓扑结构比我们的更复杂，并且特征图的空间分辨率在第一层中被更积极地降低以减少计算量。 正如将在 章节4.5中的那样。我们的模型在单网络分类精度方面优于 Szegedy 等人（2014）的模型。

@[toc]

# 3. Classification Framework

In the previous section we presented the details of our network configurations. In this section, we describe the details of classification ConvNet training and evaluation.

> 在先前章节我们展示了网络的配置细节。在这个章节里，我们将详细的介绍卷积网络分类和评估方法。

## 3.1. Training

The ConvNet training procedure generally follows Krizhevsky et al. (2012) (except for sampling the input crops from multi-scale training images, as explained later). Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et al., 1989)) with momentum. The batch size was set to 256, momentum to 0.9. 

> 卷积网络训练训练过程基本上和Krizhevsky等人(2012)一致（除了从多尺度训练图像中对输入进行采样，后面会解释）。即，使用小批量梯度下降（基于反向传播（LeCun 等人，1989））和动量优化多项逻辑回归来进行训练。批量大小被设置为256，动量为0.9。

The training was regularised by weight decay (the L2 penalty multiplier set to $5.10^{-4}$) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5). The learning rate was initially set to $10^{−2}$, and then decreased by a factor of 10 when the validation set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs). 

> 训练通过权重衰减进行正则化 （L2 惩罚乘数设置为 $5.10^{-4}$ ）以及前两个全连接层的 dropout 正则化（dropout 比例设置为0.5）。学习率初始化为 $10^{−2}$，然后在验证集准确性停止提高时降低 10 倍。总共降低了 3 次学习率，在 370K 次迭代（74 个 epoch）后停止学习。

We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al., 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers.

> 尽管与（Krizhevsky et al., 2012）相比，我们的网络有更多的参数和更大的深度，我们推测网络需要更少的迭代次数的原因是（a）由于更大的深度和更小的卷积所带来的隐式正则化；（b）以及某些层的初始化。

The initialisation of the network weights is important, since bad initialisation can stall learning due to the instability of gradient in deep nets. To circumvent this problem, we began with training the configuration A (Table 1), shallow enough to be trained with random initialisation. 

> 对网络权重的初始化是非常重要的，因为糟糕的初始化会导致深层网络梯度的不确定性而导致学习迟滞。为了规避这个问题，我们在配置A（表1）训练开始时，随机赋值以使网络得以训练。

Then, when training deeper architectures, we initialised the first four convolutional layers and the last three fully-connected layers with the layers of net A (the intermediate layers were initialised randomly). We did not decrease the learning rate for the pre-initialised layers, allowing them to change during learning. 

> 之后，在训练更深的结构时，我们初始化网络A的前4层卷积层以及后3层全链接层（至于中间部分则随机初始化）。我们不因预训练层而减少学习率，而是允许它们随训练一起进行迭代。

For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and $10^{−2}$ variance. The biases were initialised with zero. It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of Glorot & Bengio (2010).

> 为了随机初始化（当允许时），我们将权重按照标准正态分布进行采样，设置均值为0，方差为$10^{-2}$。偏差在初始时设置为0。这样做没啥价值，在我们的论文提交后，我们发现在 Glorot & Bengio (2010) 的论文里提到可以使用随机权重初始化而不是预训练。

To obtain the fixed-size 224×224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration). To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky et al., 2012). Training image rescaling is explained below.

> 为了获得固定的大小 224x224 卷积网络输入图像，他们从重缩放的训练图像中随机裁剪（在每迭代一次随机梯度下降「SGD」，对每张图像裁剪一次）。为了进一步增加训练集，裁剪的图片经历了随机水平翻转和随机缩放，以及RGB颜色随机偏移（Krizhevsky et al., 2012）。接下来会解释什么是重新缩放。

**Training image size**. Let $S$ be the smallest side of an isotropically-rescaled training image, from which the ConvNet input is cropped (we also refer to $S$ as the training scale). While the crop size is fixed to $224 \times 224$, in principle $S$ can take on any value not less than 224: for $S = 224$ the crop will capture whole-image statistics, completely spanning the smallest side of a training image; for $S \gg 224$ the crop will correspond to a small part of the image, containing a small object or an object part.

> **训练图像尺寸**。$S$ 是向同性重缩放后训练图像的最小边，裁剪后为卷积层输入大小（我们也将 $S$ 称为训练规模）。虽然裁剪大小固定为 $224 \times 224$，但原则上 $S$ 可以取不小于 224 的任何值：当 $S = 224$ 时，裁剪将得到整个图像的统计信息，并且可以完全越过训练图像的最小边；对于 $S \gg 224$的情况，将裁剪图片的一部分，包含一个小对象或对象的一部分。

We consider two approaches for setting the training scale $S$. The first is to fix $S$, which corresponds to single-scale training (note that image content within the sampled crops can still represent multi-scale image statistics). In our experiments, we evaluated models trained at two fixed scales: $S = 256$ (which has been widely used in the prior art (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014)) and $S = 384$. Given a ConvNet configuration, we first trained the network using $S = 256$. To speed-up training of the $S = 384$ network, it was initialised with the weights pre-trained with $S = 256$, and we used a smaller initial learning rate of $10^{−3}$.

> 我们考虑了两种设置训练规模 $S$ 的方法。首先限制 $S$ 的大小，它对应于单尺度训练（注意，采样后的剪切内的图像内容，仍然可以表示多尺度图像统计）。在我们的实验中，我们评估了模型在两个固定尺寸的表现：$S = 256$（在现有技术中已广泛使用（Krizhevsky 等人，2012；Zeiler & Fergus，2013；Sermanet 等人，2014））和 $S = 384$。给定一个卷积层配置，我们首先使用 $S = 256$ 训练网络。为了加速 $S = 384$ 网络的训练，它使用了 $S=256$ 的预训练权重进行初始化，我们使用了较小的初始化学习率 $10^{-3}$。

The second approach to setting $S$ is multi-scale training, where each training image is individually rescaled by randomly sampling $S$ from a certain range [$S_{min}$, $S_{max}$] (we used $S_{min} = 256$ and $S_{max} = 512$). Since objects in images can be of different size, it is beneficial to take this into account during training. This can also be seen as training set augmentation by scale jittering, where a single model is trained to recognise objects over a wide range of scales. For speed reasons, we trained multi-scale models by fine-tuning all layers of a single-scale model with the same configuration, pre-trained with fixed $S = 384$.

> 设置 $S$ 的第二种方法是多尺度训练，即每个训练图像通过从范围 [ $S_{min}$, $S_{max}$ ] 中随机采样 $S$ 来单独重新缩放（我们使用 $S_{min} = 256$ 和 $S_{max} = 512$)。 由于图像中的对象可以具有不同的大小，因此在训练期间考虑到这一点是有益的。 这也可以看作是通过尺度抖动来增加训练集，其中训练单个模型以识别各种尺度上的对象。 出于速度原因，我们通过微调具有相同配置的单尺度模型的所有层来训练多尺度模型，并使用固定的 $S = 384$ 进行预训练。


## 3.2. Testing 

At test time, given a trained ConvNet and an input image, it is classified in the following way. First, it is isotropically rescaled to a predefined smallest image side, denoted as Q (we also refer to it as the test scale). We note that Q is not necessarily equal to the training scale S (as we will show in Sect. 4, using several values of Q for each S leads to improved performance). Then, the network is applied densely over the rescaled test image in a way similar to (Sermanet et al., 2014). 

> 在测试环节，给训练后的卷积网络一张输入图像，它可以被下面的流程识别到。首先，它被各向同性地重新缩放到预定义的最小图像大小，表示为Q（我们通常定义它为测试尺度）。我们注意到Q并不需要和训练尺度S保持一致（在章节4里，我们会对每一个S用不同的Q，以提升性能）。然后，网络以类似于 (Sermanet et al., 2014) 的方式密集应用在重新缩放的测试图像上。

Namely, the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 × 7 conv. layer, the last two FC layers to 1 × 1 conv. layers). The resulting fully-convolutional net is then applied to the whole (uncropped) image. The result is a class score map with the number of channels equal to the number of classes, and a variable spatial resolution, dependent on the input image size. Finally, to obtain a fixed-size vector of class scores for the image, the class score map is spatially averaged (sum-pooled). We also augment the test set by horizontal flipping of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image.

> 即，首先将全连接层转换为卷积层（第一个 FC 层为 7×7 卷积层，最后两个 FC 层为 1×1 卷积层）。 然后将得到的全卷积网络应用于整个（未裁剪）图像。 结果是一个类分数图，其通道数等于类数，以及一个取决于输入图像的大小的可变的空间分辨率。最后，为了获得固定大小的类别分数向量，对类别分数图进行空间平均（总和池化）。 我们还通过图像的水平翻转来增加测试集； 对原始图像和翻转图像的 soft-max 类后验进行平均以获得图像的最终分数。

Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time (Krizhevsky et al., 2012), which is less efficient as it requires network re-computation for each crop. At the same time, using a large set of crops, as done by Szegedy et al. (2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared to the fully-convolutional net. 

> 由于全卷积网络应用于整个图像，因此无需在测试时对多个剪切图进行采样（Krizhevsky et al., 2012），因为它需要网络对每个剪切进行重新计算，因此效率较低。 同时，使用大量的剪切图，如 Szegedy 等人（2014）所做的那样，可以提高准确性，因为与全卷积网络相比，它可以对输入图像进行更精细的采样。

Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured. 

> 此外，由于不同的卷积边界条件，多剪切评估与密集评估是互补的：当将卷积网络应用于剪切图时，卷积特征用零填充，而在密集评估的情况下，相同的剪切的填充自然会出现来自图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。

While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 × 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by Szegedy et al. (2014).

> 虽然我们认为在实践中，增加多个剪切图的计算时间并不能证明潜在的准确性提高是合理的，但作为参考，我们还使用每个尺度 50 个剪切图（5 × 5 规则网格和 2 次翻转）评估我们的网络，总共 150 超过 3 个尺度的剪切图，与 Szegedy 等人（2014）使用的 4 个尺度的 144 个剪切图相当。

## 3.3. Implementation Details


Our implementation is derived from the publicly available C++ Caffe toolbox (Jia, 2013) (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above). Multi-GPU training exploits data parallelism, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the GPUs, so the result is exactly the same as when training on a single GPU.

> 我们的代码实现源自公开可用的 C++ Caffe 工具箱 (Jia, 2013)（于 2013 年 12 月推出），但包含许多重大修改，允许我们在单个系统中安装的多个 GPU 上执行训练和评估，以及 训练和评估多尺度的全尺寸（未裁剪）图像（如上所述）。 多 GPU 训练利用数据并行性，通过将每批训练图像分成几个 GPU 批次，在每个 GPU 上并行处理来进行。 在计算 GPU 批次梯度后，对它们执行平均计算以获得完整的梯度。 梯度计算在 GPU 之间是同步的，因此结果与在单个 GPU 上训练时的结果完全相同。

While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU. On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.

> 虽然最近提出了更复杂的加速卷积网络训练的方法 (Krizhevsky, 2014)，它们对网络的不同层采用模型和数据并行性，但我们发现与使用单个 GPU 相比，在现成的 4-GPU 系统上，我们的方案已经提供了 3.75 倍的加速。 在配备四个 NVIDIA Titan Black GPU 的系统上，训练单个网络需要 2-3 周，具体取决于架构。


# 4. Classification Experiments 

**Dataset**. In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 chal- lenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels). The clas- sification performance is evaluated using two measures: the top-1 and top-5 error. The former is a multi-class classification error, i.e. the proportion of incorrectly classified images; the latter is the main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories.

> **数据集**。 在本节中，我们展示了由卷积网络架构在 ILSVRC-2012 数据集（用于 ILSVRC 2012-2014 挑战）上实现的图像分类结果。 该数据集包括 1000 个类别的图像，分为三组：训练（130 万张图像）、验证（5 万张图像）和测试（10 万张带有保留类标签的图像）。 分类性能使用两个度量来评估：top-1 和 top-5 错误。 前者是多类分类错误，即错误分类图像的比例； 后者是 ILSVRC 中使用的主要评估标准，计算为真实类别超出前 5 个预测类别的图像比例。

For the majority of experiments, we used the validation set as the test set. Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a “VGG” team entry to the ILSVRC-2014 competition (Russakovsky et al., 2014).

> 对于大多数实验，我们使用验证集作为测试集。 某些实验也在测试集上进行，并提交给官方 ILSVRC 服务器，作为 ILSVRC-2014 比赛的“VGG”团队参赛作品（Russakovsky et al., 2014）。


## 4.1 SINGLE SCALE EVALUATION

We begin with evaluating the performance of individual ConvNet models at a single scale with the layer configurations described in Sect. 2.2. The test image size was set as follows: $Q = S$ for fixed S, and $Q = 0.5(S_{min} + S_{max})$ for jittered $S \in [S_{min}, S_{max}]$. The results of are shown in Table 3.

> 我们首先使用在章节2.2中描述的层配置，在单一尺度上评估单个卷积网络模型的性能。 测试图像大小设置如下：$Q = S$，固定 S， $Q = 0.5(S_{min} + S_{max})$，抖动 $S \in [S_{min}, S_{max}]$。 结果如表3所示。

First, we note that using local response normalisation (A-LRN network) does not improve on the model A without any normalisation layers. We thus do not employ normalisation in the deeper architectures (B–E).

> 首先，我们注意到使用局部响应归一化（A-LRN 网络）并没有改进没有任何归一化层的模型 A。 因此，我们不在更深层次的架构（B-E）中使用归一化。

Second, we observe that the classification error decreases with the increased ConvNet depth: from 11 layers in A to 19 layers in E. Notably, in spite of the same depth, the configuration C (which contains three 1 × 1 conv. layers), performs worse than the configuration D, which uses 3 × 3 conv. layers throughout the network. 

> 其次，我们观察到随着卷积网络深度的增加分类错误会逐渐减少：从 A 中的 11 层到 E 中的 19 层。值得注意的是，尽管深度相同，配置 C（包含三个 1×1 卷积层）， 性能比使用 3 × 3卷积网络层的配置 D 更差。

This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial receptive fields (D is better than C). The error rate of our architecture saturates when the depth reaches 19 layers, but even deeper models might be beneficial for larger datasets. 

> 这表明虽然额外的非线性确实有帮助（C 优于 B），但使用卷积滤波非平凡感受器（D优于C）同样对于捕获空间上下文信息也很重要。我们架构的错误率在深度达到19层时达到饱和，但更深的模型可能对更大的数据集有益。

We also compared the net B with a shallow net with five 5 × 5 conv. layers, which was derived from B by replacing each pair of 3 × 3 conv. layers with a single 5 × 5 conv. layer (which has the same receptive field as explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters.

> 我们还将网络 B 与包含了5个 5 × 5 的卷积层浅网络进行了比较，它是从B中，通过替换每对 3 × 3 卷积派生的，具有单个 5 × 5的卷积层（具有与第 2.3 节中解释的相同的感受器）。 浅网的 top-1 误差被测量为比 B 高 7%（在中心剪切图片上），这证实了带有小过滤器的深网优于带有较大过滤器的浅网。

Finally, scale jittering at training time $(S \in [256; 512])$ leads to significantly better results than training on images with fixed smallest side $(S = 256 or S = 384)$, even though a single scale is used at test time. This confirms that training set augmentation by scale jittering is indeed helpful for capturing multi-scale image statistics.

> 最后，训练时的尺度抖动$(S \in [256; 512])$ 比在具有固定最小边的图像 $(S = 256 or S = 384)$上训练的结果要好得多，即使在测试时使用单个尺度。 这证实了通过尺度抖动增加训练集确实有助于捕获多尺度图像统计信息。
> 
![在这里插入图片描述](https://img-blog.csdnimg.cn/3da573188d0541f08d4d7ebc1bd8e420.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)
## 4.2. MULTI-SCALE EVALUATION

Having evaluated the ConvNet models at a single scale, we now assess the effect of scale jittering at test time. It consists of running a model over several rescaled versions of a test image (corresponding to different values of Q), followed by averaging the resulting class posteriors. 

> 在单一尺度上评估了卷积网络模型后，我们现在评估在测试时尺度抖动的影响。 它包括在测试图像的几个重新缩放版本（对应于不同的 Q 值），然后平均得到的类后验。 

Considering that a large discrepancy between training and testing scales leads to a drop in performance, the models trained with fixed S were evaluated over three test image sizes, close to the training one: Q = {S − 32, S, S + 32}. At the same time, scale jittering at training time allows the network to be applied to a wider range of scales at test time, so the model trained with variable $S \in [S_{min}; S_{max}]$was evaluated over a larger range of sizes $Q = \{S_{min}, 0.5(S_{min} + S_{max}), S_{max} \}$.

> 考虑到训练和测试规模之间的巨大差异会导致性能下降，使用固定 S 训练的模型在三个测试图像大小上进行了评估，接近训练一个：Q = {S − 32, S, S + 32} . 同时，训练时的尺度抖动使得网络在测试时可以应用到更广泛的尺度上，所以用变量  $S \in [S_{min}; S_{max}]$ 训练模型，并在更大范围上 $Q = \{S_{min}, 0.5(S_{min} + S_{max}), S_{max} \}$ 做评估。

The results, presented in Table 4, indicate that scale jittering at test time leads to better performance (as compared to evaluating the same model at a single scale, shown in Table 3). As before, the deepest configurations (D and E) perform the best, and scale jittering is better than training with a fixed smallest side S.

> 表 4 中显示的结果表明，测试时的尺度抖动会带来更好的性能（与在单一尺度上评估相同模型相比，如表 3 所示）。 和以前一样，最深的配置（D 和 E）表现最好，尺度抖动优于使用固定的最小边 S 进行训练。

Our best single-network performance on the validation set is 24.8%/7.5% top-1/top-5 error (highlighted in bold in Table 4). On the test set, the configuration E achieves 7.3% top-5 error.

> 在验证集上，我们的单网络性能表现为 24.8%/7.5%  top-1/top -5 error（在表 4 中以粗体突出显示），在测试集上，配置 E 实现了 7.3% 的 top-5 错误。

![在这里插入图片描述](https://img-blog.csdnimg.cn/465b13403c61419e94f49ae173169171.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)
## 4.3. MULTI-CROP EVALUATIO

In Table 5 we compare dense ConvNet evaluation with mult-crop evaluation (see Sect. 3.2 for details). We also assess the complementarity of the two evaluation techniques by averaging their soft- max outputs. As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed complementary, as their combination outperforms each of them. As noted above, we hypothesize that this is due to a different treatment of convolution boundary conditions.

> 在表 5 中，我们将密集的卷积网络评估与多剪切图评估进行了比较（详见第 3.2 节）。 我们还通过平均它们的 softmax 输出来评估这两种评估技术的互补性。 可以看出，使用多个剪切图的性能略好于密集评估，并且这两种方法确实是互补的，因为它们的组合优于它们中的每一种。 如上所述，我们假设这是由于对卷积边界条件的不同处理。

![在这里插入图片描述](https://img-blog.csdnimg.cn/e7f2950673a44401be977395359f0034.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

## 4.4. CONVNET FUSION

Up until now, we evaluated the performance of individual ConvNet models. In this part of the experiments, we combine the outputs of several models by averaging their soft-max class posteriors. This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al., 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014).

> 到目前为止，我们评估了单个卷积网络模型的性能。 在这部分实验中，我们通过平均它们的 soft-max 类后验来组合几个模型的输出。 由于模型的互补性，这提高了性能，并在 2012 年（Krizhevsky 等人，2012 年）和 2013 年（Zeiler & Fergus，2013 年；Sermanet 等人，2014 年）的顶级 ILSVRC 提交中使用。

The results are shown in Table 6. By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers). The resulting ensemble of 7 networks has 7.3% ILSVRC test error. After the submission, we considered an ensemble of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation. For reference, our best-performing single model achieves 7.1% error (model E, Table 5).

> 结果如表 6 所示。到提交 ILSVRC 时，我们只训练了单尺度网络以及多尺度模型 D（通过仅微调全连接层而不是所有层）。 由此产生的 7 个网络的集合具有 7.3% 的 ILSVRC 测试误差。 提交后，我们考虑了只有两个性能最佳的多尺度模型（配置 D 和 E）的集合，使用密集评估将测试误差降低到 7.0%，使用组合密集和多作物评估将测试误差降低到 6.8%。 作为参考，我们表现最好的单一模型实现了 7.1% 的误差（模型 E，表 5）。

## 4.5. COMPARISON WITH THE STATE OF THE ART

Finally, we compare our results with the state of the art in Table 7. In the classification task of ILSVRC-2014 challenge (Russakovsky et al., 2014), our “VGG” team secured the 2nd place with 7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to 6.8% using an ensemble of 2 models.

> 最后，我们将我们的结果与表 7 中的最新技术进行比较。在 ILSVRC-2014 挑战赛的分类任务（Russakovsky 等人，2014 年）中，我们的“VGG”团队使用 7个模型的合奏。 提交后，我们使用 2 个模型的集合将错误率降低到 6.8%。
![在这里插入图片描述](https://img-blog.csdnimg.cn/ed976c75a0b14add84d056935f736da9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

As can be seen from Table 7, our very deep ConvNets significantly outperform the previous gener- ation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competi- tions. Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it. This is remarkable, considering that our best result is achieved by combining just two models – significantly less than used in most ILSVRC submissions. In terms of the single-net performance, our architecture achieves the best result (7.0% test error), outperforming a single GoogLeNet by 0.9%. Notably, we did not depart from the classical ConvNet architecture of LeCun et al. (1989), but improved it by substantially increasing the depth.

> 从表 7 可以看出，我们非常深的 ConvNets 显着优于上一代模型，在 ILSVRC-2012 和 ILSVRC-2013 比赛中取得了最好的成绩。 我们的结果在分类任务获胜者（GoogLeNet 的错误率为 6.7%）方面也具有竞争力，并且大大优于 ILSVRC-2013 获胜提交的 Clarifai，后者在使用外部训练数据的情况下达到 11.2%，在没有外部训练数据的情况下达到 11.7%。 这是了不起的，考虑到我们的最佳结果是通过仅组合两个模型来实现的——明显少于大多数 ILSVRC 提交中使用的模型。 在单网性能方面，我们的架构取得了最好的结果（7.0% 的测试错误），比单个 GoogLeNet 高 0.9%。 值得注意的是，我们并没有背离 LeCun 等人的经典 ConvNet 架构。 （1989），但通过显着增加深度对其进行了改进。

![在这里插入图片描述](https://img-blog.csdnimg.cn/f446ca43d3ad4e1d8ab7875996c71c10.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

# 5. CONCLUSION

In this work we evaluated very deep convolutional networks (up to 19 weight layers) for large- scale image classification. It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. In the appendix, we also show that our models generalise well to a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines built around less deep image representations. Our results yet again confirm the importance of depth in visual representations.

> 在这项工作中，我们评估了用于大规模图像分类的非常深的卷积网络（最多 19 个权重层）。 已经证明，表示深度有利于分类准确性，并且可以使用传统的 ConvNet 架构在 ImageNet 挑战数据集上实现最先进的性能（LeCun 等人，1989；Krizhevsky 等人， 2012），深度显着增加。 在附录中，我们还展示了我们的模型可以很好地推广到广泛的任务和数据集，匹配或优于围绕较少深度图像表示构建的更复杂的识别管道。 我们的结果再次证实了深度在视觉表示中的重要性。

# ACKNOWLEDGEMENTS

This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.

# REFERENCES
* Bell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials in context database. CoRR, abs/1412.0623, 2014.
* Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep into convolutional nets. In Proc. BMVC., 2014.
* Cimpoi, M., Maji, S., and Vedaldi, A. Deep convolutional filter banks for texture recognition and segmentation. CoRR, abs/1411.6836, 2014.
* Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance convolutional neural networks for image classification. In IJCAI, pp. 1237–1242, 2011. 
* Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., Le, Q. V., and Ng, A. Y. Large scale distributed deep networks. In NIPS, pp. 1232–1240, 2012.
* Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proc. CVPR, 2009. 
* Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. CoRR, abs/1310.1531, 2013.
* Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C., Winn, J., and Zisserman, A. The Pascal visual object classes challenge: A retrospective. IJCV, 111(1):98–136, 2015. 
* Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In IEEE CVPR Workshop of Generative Model Based Vision, 2004. 
* Girshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR, abs/1311.2524v5, 2014. Published in Proc. CVPR, 2014. 
* Gkioxari, G., Girshick, R., and Malik, J. Actions and attributes from wholes and parts. CoRR, abs/1412.2604, 2014. 
* Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc. AISTATS, volume 9, pp. 249–256, 2010. 
* Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street view imagery using deep convolutional neural networks. In Proc. ICLR, 2014. 
* Griffin, G., Holub, A., and Perona, P. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007. 
* He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid pooling in deep convolutional networks for visual recognition. CoRR, abs/1406.4729v2, 2014. 
* Hoai, M. Regularized max pooling for image categorization. In Proc. BMVC., 2014. 
* Howard, A. G. Some improvements on deep convolutional neural network based image classification. In Proc. ICLR, 2014. 
* Jia, Y. Caffe: An open source convolutional architecture for fast feature embedding. 
* http://caffe.berkeleyvision.org/, 2013.
* Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. CoRR,abs/1412.2306, 2014. 
* Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural language models. CoRR, abs/1411.2539, 2014.
* Krizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997, 2014.
* Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural net- works. In NIPS, pp. 1106–1114, 2012.
* LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropa- gation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.
* Lin, M., Chen, Q., and Yan, S. Network in network. In Proc. ICLR, 2014. 
* Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation. CoRR, abs/1411.4038, 2014.
* Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks. In Proc. CVPR, 2014.
* Perronnin, F., Sa ́nchez, J., and Mensink, T. Improving the Fisher kernel for large-scale image classification. In Proc. ECCV, 2010.
* Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. CNN Features off-the-shelf: an Astounding Baseline for Recognition. CoRR, abs/1403.6382, 2014.
* Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014.
* Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In Proc. ICLR, 2014.
* Simonyan, K. and Zisserman, A. Two-stream convolutional networks for action recognition in videos. CoRR, abs/1406.2199, 2014. Published in Proc. NIPS, 2014.
* Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. CoRR, abs/1409.4842, 2014.
* Wei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan, S. CNN: Single-label to multi-label. CoRR, abs/1406.5726, 2014.
* Zeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901, 2013. Published in Proc. ECCV, 2014.

@[toc]

# A LOCALISATION
In the main body of the paper we have considered the classification task of the ILSVRC challenge, and performed a thorough evaluation of ConvNet architectures of different depth. In this section, we turn to the localisation task of the challenge, which we have won in 2014 with 25.3% error. It can be seen as a special case of object detection, where a single object bounding box should be predicted for each of the top-5 classes, irrespective of the actual number of objects of the class. For this we adopt the approach of Sermanet et al. (2014), the winners of the ILSVRC-2013 localisation challenge, with a few modifications. Our method is described in Sect. A.1 and evaluated in Sect. A.2.

> 在文章的主要部分，我们比较了ILSVRC的分类挑战，通过对卷积网络在不同深度的表现，进行了详细的评估。在本章节，我们瞄准了定位任务，并且在2014年的比赛种，我们以25.3%的错误率赢得比赛。定位任务可以被视为一类特殊的物体识别，为top-5类别的的每个类别分别预测，并且与实际数量无关。因此，我们采取了Sermanet et al. (2014) 的方法，他们是ILSVRC-2013定位比赛的冠军，我们在他们的方法上做了很小的改动。具体细节在A.1节，评估在A.2节。

## A.1 LOCALISATION CONVNET
To perform object localisation, we use a very deep ConvNet, where the last fully connected layer predicts the bounding box location instead of the class scores. A bounding box is represented by a 4-D vector storing its center coordinates, width, and height. There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, SCR (Sermanet et al., 2014)) or is class-specific (per-class regression, PCR). In the former case, the last layer is 4-D, while in the latter it is 4000-D (since there are 1000 classes in the dataset). Apart from the last bounding box prediction layer, we use the ConvNet architecture D (Table 1), which contains 16 weight layers and was found to be the best-performing in the classification task (Sect. 4). 

> 为了执行物体识别，我们使用一个非常深的卷积网络，在网络最后的全连接层，它负责预测bounding box的位置而不是，分类置信度。bounding box的详细信息由一个4D向量表示，分别表示中心点坐标（x，y），宽(w)、高(h)。我们可以决定bounding box是否作用于全部类别（single-class 聚类，SCR（Sermanet et al., 2014)））或特定类型（per-class 聚类，PCR）。在前一种情况，最后层是一个4-D，在之后是4000-D（例如，数据集中有1000 类别）。除了最后一层用于预测bounding box，我们使用了和结构D（表1）一样的结构，它包含16个权重层，并发现它在分类任务中表现出众（章节4）。


**Training.** Training of localisation ConvNets is similar to that of the classification ConvNets (Sect. 3.1). The main difference is that we replace the logistic regression objective with a Euclidean loss, which penalises the deviation of the predicted bounding box parameters from the ground-truth. We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale jittering for our ILSVRC-2014 submission). Training was initialised with the corresponding classification models (trained on the same scales), and the initial learning rate was set to $10^{−3}$ . We explored both fine-tuning all layers and fine-tuning only the first two fully-connected layers, as done in (Sermanet et al., 2014). The last fully-connected layer was initialised randomly and trained from scratch.

> **训练。** 卷积定位网络和卷积分类网络（章节3.1）的训练过程相似。它们之间最主要的区别在于，我们将逻辑回归替换为欧氏距离损失，它用于惩罚 bounding box和真实值的偏差。我们训练了两种定位模型，每个都有不同的尺度：S=256 和 S= 384（由于时间限制，我们没有在 ILSVRC-201 提交的论文里使用尺度抖动）。使用相应的分类模型（基于同样尺度训练）初始化训练，并设置学习率为$10^{−3}$。我们研究了如 (Sermanet et al., 2014) 种描述的，分别微调所有层和仅微调两个全连接层。最后的全连接层随机初始化，并从头训练。


**Testing.** We consider two testing protocols. The first is used for comparing different network modifications on the validation set, and considers only the bounding box prediction for the ground truth class (to factor out the classification errors). The bounding box is obtained by applying the network only to the central crop of the image. 

> **测试。** 我们考虑两种测试协议。第一种用于比较不同的网络在验证集上的表现，并只考虑bounding box在对ground truth类型的预测上（用于找出分类错误）。边界框是通过将网络仅应用于图像的中心裁剪来获得的。

The second, fully-fledged, testing procedure is based on the dense application of the localisation ConvNet to the whole image, similarly to the classification task (Sect. 3.2). The difference is that instead of the class score map, the output of the last fully-connected layer is a set of bounding box predictions. To come up with the final prediction, we utilise the greedy merging procedure of Sermanet et al. (2014), which first merges spatially close predictions (by averaging their coordinates), and then rates them based on the class scores, obtained from the classification ConvNet. When several localisation ConvNets are used, we first take the union of their sets of bounding box predictions, and then run the merging procedure on the union. We did not use the multiple pooling offsets technique of Sermanet et al. (2014), which increases the spatial resolution of the bounding box predictions and can further improve the results. 

> 第二个成熟的、测试过程，是基于图像的卷积定位网络密集应用，类似于分类任务（第 3.2 节）。 不同之处在于，最后一个全连接层的输出不是类分数图，而是一组边界框预测。 为了得出最终预测，我们利用了 Sermanet 等人（2014）的贪婪合并程序。 它首先合并空间上接近的预测（使用平均坐标），然后根据从卷积分类网络获得的类分数对它们进行评分。 当使用多个卷积定位网络时，我们首先取它们的bounding box测集的并集，然后在并集上运行合并过程。 我们没有使用Sermanet 等人（2014）的多重池化偏移技术。 这可以增加bounding box预测的空间分辨率，并可以进一步改善结果。

## A.2 LOCALISATION EXPERIMENTS
In this section we first determine the best-performing localisation setting (using the first test protocol), and then evaluate it in a fully-fledged scenario (the second protocol). The localisation error is measured according to the ILSVRC criterion (Russakovsky et al., 2014), i.e. the bounding box prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box is above 0.5.

> 在本节中，我们首先确定最佳的定位设置（使用第一种方法），之后评估它在在完全状态的表现（第二种方法）。定位误差可以根据ILSVRC标准（Russakovsky et al., 2014）测量得到。例如，认为bounding box与真实边界框的交并比大于 0.5，则认为bounding box预测是正确的。

**Settings comparison.** As can be seen from Table 8, per-class regression (PCR) outperforms the class-agnostic single-class regression (SCR), which differs from the findings of Sermanet et al. (2014), where PCR was outperformed by SCR. We also note that fine-tuning all layers for the localisation task leads to noticeably better results than fine-tuning only the fully-connected layers (as done in (Sermanet et al., 2014)). In these experiments, the smallest images side was set to S = 384; the results with S = 256 exhibit the same behaviour and are not shown for brevity.

> **设置比较。** 从表 8 中可以看出，类回归 (PCR) 优于类不可知单类回归 (SCR)，这与 Sermanet 等人（2014）的发现不同。 PCR 的表现优于 SCR。 我们还注意到，为定位任务微调所有层比仅微调全连接层（如 (Sermanet et al., 2014) 中所做的那样）产生明显更好的结果。 在这些实验中，最小图像侧设置为 S = 384；S = 256 的结果表现出相同的行为，为简洁起见未显示。

Table 8: **Localisation error for different modifications** with the simplified testing protocol: the bounding box is predicted from a single central image crop, and the ground-truth class is used. All ConvNet layers (except for the last one) have the configuration D (Table 1), while the last layer performs either single-class regression (SCR) or per-class regression (PCR).

> 表 8：**不同修改的定位误差**使用简化的测试协议：边界框是从单个中心图像裁剪中预测的，并且使用了 ground-truth 类。 所有 ConvNet 层（最后一层除外）都具有配置 D（表 1），而最后一层执行单类回归 (SCR) 或每类回归 (PCR)。

![在这里插入图片描述](https://img-blog.csdnimg.cn/ffc32a362b0141b4842dca09acad24f2.png#pic_center)
**Fully-fledged evaluation.** Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014). As can be seen from Table 9, application of the localisation ConvNet to the whole image substantially improves the results compared to using a center crop (Table 8), despite using the top-5 predicted class labels instead of the ground truth. Similarly to the classification task (Sect. 4), testing at several scales and combining the predictions of multiple networks further improves the performance.

> **完全成熟的评估。** 确定了最佳定位设置（PCR，所有层的微调）后，我们现在将其应用到完全成熟的场景中，其中前 5 个类别标签是使用我们的 性能最佳的分类系统（第 4.5 节），并使用 Sermanet 等人（2014）的方法合并多个密集计算的边界框预测。 从表 9 可以看出，与使用中心裁剪相比（表 8），将定位 ConvNet 应用于整个图像显着提高了结果，尽管使用前 5 个预测类标签而不是基本事实。 与分类任务（第 4 节）类似，在多个尺度上进行测试并结合多个网络的预测进一步提高了性能。

![在这里插入图片描述](https://img-blog.csdnimg.cn/e2de24575b4948ed83fe03f3eb1956da.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center)
**Comparison with the state of the art.** We compare our best localisation result with the state of the art in Table 10. With 25.3% test error, our “VGG” team won the localisation challenge of ILSVRC-2014 (Russakovsky et al., 2014). Notably, our results are considerably better than those of the ILSVRC-2013 winner Overfeat (Sermanet et al., 2014), even though we used less scales and did not employ their resolution enhancement technique. We envisage that better localisation performance can be achieved if this technique is incorporated into our method. This indicates the performance advancement brought by our very deep ConvNets – we got better results with a simpler localisation method, but a more powerful representation.

> **与最先进水平的比较。** 我们在表 10 中将我们的最佳定位结果与最先进水平进行了比较。以 25.3% 的测试错误率，我们的“VGG”团队赢得了 ILSVRC-2014 的定位挑战（Russakovskyet al，2014）。 值得注意的是，我们的结果比 ILSVRC-2013 获胜者 Overfeat（Sermanet et al，2014 年）的结果要好得多，尽管我们使用了较少的尺度并且没有使用他们的分辨率增强技术。 我们设想如果将这种技术结合到我们的方法中，可以实现更好的定位性能。 这表明我们非常深的 ConvNets 带来了性能提升——我们使用更简单的定位方法获得了更好的结果，但表示更强大。

# B GENERALISATION OF VERY DEEP FEATURES

In the previous sections we have discussed training and evaluation of very deep ConvNets on the ILSVRC dataset. In this section, we evaluate our ConvNets, pre-trained on ILSVRC, as feature Table 10: Comparison with the state of the art in ILSVRC localisation. Our method is denoted as “VGG”

> 在前面的部分中，我们讨论了在 ILSVRC 数据集上训练和评估非常深的 ConvNet。 在本节中，我们评估了在 ILSVRC 上预训练的 ConvNets，作为特征表 10：与 ILSVRC 本地化的最新技术的比较。 我们的方法表示为“VGG”

![在这里插入图片描述](https://img-blog.csdnimg.cn/ba60f575829e473ca10cc78905100ff0.png#pic_center)

extractors on other, smaller, datasets, where training large models from scratch is not feasible due to over-fitting. Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al., 2013; Razavian et al., 2014; Chatfield et al., 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin. Following that line of work, we investigate if our models lead to better performance than more shallow models utilised in the state-of-the-art methods. In this evaluation, we consider two models with the best classification performance on ILSVRC (Sect. 4) – configurations “Net-D” and “Net-E” (which we made publicly available).

> 其他较小的数据集上的提取器，由于过度拟合，从头开始训练大型模型是不可行的。 最近，人们对这种用例很感兴趣（Zeiler & Fergus，2013；Donahue et al，2013；Razavian et al，2014；Chatfield et al，2014），因为事实证明，深度图像 在 ILSVRC 上学习的表示可以很好地推广到其他数据集，在这些数据集上它们的性能大大优于手工制作的表示。 在这项工作之后，我们调查我们的模型是否比最先进的方法中使用的更浅的模型具有更好的性能。 在本次评估中，我们考虑了两个在 ILSVRC（第 4 节）上具有最佳分类性能的模型——配置“Net-D”和“Net-E”（我们公开提供）。

To utilise the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D activations of the penultimate layer as image features, which are aggregated across multiple locations and scales. The resulting image descriptor is L2-normalised and combined with a linear SVM classifier, trained on the target dataset. For simplicity, pre-trained ConvNet weights are kept fixed (no fine-tuning is performed).

> 为了利用在 ILSVRC 上预训练的 ConvNets 在其他数据集上进行图像分类，我们删除了最后一个全连接层（执行 1000 路 ILSVRC 分类），并使用倒数第二层的 4096-D 激活作为图像特征， 它们是跨多个位置和规模聚合的。 生成的图像描述符是 L2 归一化的，并与线性 SVM 分类器相结合，在目标数据集上进行训练。 为简单起见，预训练的 ConvNet 权重保持固定（不执行微调）。

Aggregation of features is carried out in a similar manner to our ILSVRC evaluation procedure (Sect. 3.2). Namely, an image is first rescaled so that its smallest side equals Q, and then the network is densely applied over the image plane (which is possible when all weight layers are treated as convolutional). We then perform global average pooling on the resulting feature map, which produces a 4096-D image descriptor. The descriptor is then averaged with the descriptor of a horizontally flipped image. As was shown in Sect. 4.2, evaluation over multiple scales is beneficial, so we extract features over several scales Q. The resulting multi-scale features can be either stacked or pooled across scales. Stacking allows a subsequent classifier to learn how to optimally combine image statistics over a range of scales; this, however, comes at the cost of the increased descriptor dimensionality. We return to the discussion of this design choice in the experiments below. We also assess late fusion of features, computed using two networks, which is performed by stacking their respective image descriptors.

> 特征的聚合以与我们的 ILSVRC 评估程序（第 3.2 节）类似的方式进行。即，首先对图像进行重新缩放，使其最小边等于 Q，然后将网络密集应用到图像平面上（当所有权重层都被视为卷积时，这是可能的）。然后，我们对生成的特征图执行全局平均池化，生成 4096-D 图像描述符。然后将描述符与水平翻转图像的描述符进行平均。如 Sect 所示。 4.2，多尺度评估是有益的，因此我们提取多个尺度Q的特征。得到的多尺度特征可以跨尺度堆叠或汇集。 Stacking 允许后续分类器学习如何在一定范围内优化组合图像统计信息；然而，这是以增加描述符维度为代价的。我们在下面的实验中回到对这种设计选择的讨论。我们还评估了使用两个网络计算的特征的后期融合，这是通过堆叠它们各自的图像描述符来执行的。

Table 11: **Comparison with the state of the art in image classification on VOC-2007, VOC-2012, Caltech-101, and Caltech-256.** Our models are denoted as “VGG”. Results marked with * were achieved using ConvNets pre-trained on the extended ILSVRC dataset (2000 classes).

> 表 11：**在 VOC-2007、VOC-2012、Caltech-101 和 Caltech-256 上与现有技术的图像分类比较。**我们的模型表示为“VGG”。 标有 * 的结果是使用在扩展的 ILSVRC 数据集（2000 个类）上预训练的 ConvNets 实现的。

![在这里插入图片描述](https://img-blog.csdnimg.cn/a5a655f984e64e949c63571b6802b3f5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQWtpIFVud3ppaQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

Image Classification on VOC-2007 and VOC-2012. We begin with the evaluation on the image classification task of PASCAL VOC-2007 and VOC-2012 benchmarks (Everingham et al., 2015). These datasets contain 10K and 22.5K images respectively, and each image is annotated with one or several labels, corresponding to 20 object categories. The VOC organisers provide a pre-defined split into training, validation, and test data (the test data for VOC-2012 is not publicly available; instead, an official evaluation server is provided). Recognition performance is measured using mean average precision (mAP) across classes.

> VOC-2007 和 VOC-2012 上的图像分类。 我们从评估 PASCAL VOC-2007 和 VOC-2012 基准的图像分类任务开始（Everingham et al., 2015）。 这些数据集分别包含 10K 和 22.5K 图像，每张图像都标注了一个或多个标签，对应 20 个对象类别。 VOC 组织者提供了预定义的训练、验证和测试数据拆分（VOC-2012 的测试数据不公开；相反，提供了官方评估服务器）。 识别性能是使用跨类的平均精度 (mAP) 来衡量的。

Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we found that aggregating image descriptors, computed at multiple scales, by averaging performs similarly to the aggregation by stacking. We hypothesize that this is due to the fact that in the VOC dataset the objects appear over a variety of scales, so there is no particular scale-specific semantics which a classifier could exploit. Since averaging has a benefit of not inflating the descriptor dimensionality, we were able to aggregated image descriptors over a wide range of scales: Q ∈ {256, 384, 512, 640, 768}. It is worth noting though that the improvement over a smaller range of {256, 384, 512} was rather marginal (0.3%).

> 值得注意的是，通过检查 VOC-2007 和 VOC-2012 验证集的性能，我们发现聚合图像描述符，在多个尺度上计算，通过平均执行类似于通过堆叠聚合。 我们假设这是因为在 VOC 数据集中，对象出现在各种尺度上，因此没有分类器可以利用的特定尺度语义。 由于平均具有不膨胀描述符维度的好处，我们能够在广泛的范围内聚合图像描述符：Q ∈ {256, 384, 512, 640, 768}。 值得注意的是，在 {256, 384, 512} 的较小范围内的改进相当微不足道（0.3%）。

The test set performance is reported and compared with other approaches in Table 11. Our networks “Net-D” and “Net-E” exhibit identical performance on VOC datasets, and their combination slightly improves the results. Our methods set the new state of the art across image representations, pretrained on the ILSVRC dataset, outperforming the previous best result of Chatfield et al. (2014) by more than 6%. It should be noted that the method of Wei et al. (2014), which achieves 1% better mAP on VOC-2012, is pre-trained on an extended 2000-class ILSVRC dataset, which includes additional 1000 categories, semantically close to those in VOC datasets. It also benefits from the fusion with an object detection-assisted classification pipeline.

> 在表 11 中报告了测试集的性能并与其他方法进行了比较。我们的网络“Net-D”和“Net-E”在 VOC 数据集上表现出相同的性能，它们的组合略微提高了结果。 我们的方法设置了跨图像表示的最新技术，在 ILSVRC 数据集上进行了预训练，优于 Chatfield 等人（2014 ）之前的最佳结果超过 6%。 需要注意的是 （Wei et al. 2014）的方法。 在 VOC-2012 上实现了 1% 更好的 mAP，在扩展的 2000 类 ILSVRC 数据集上进行了预训练，该数据集包括额外的 1000 个类别，语义上接近 VOC 数据集中的类别。 它还受益于与对象检测辅助分类管道的融合。

**Image Classification on Caltech-101 and Caltech-256.** In this section we evaluate very deep features on Caltech-101 (Fei-Fei et al., 2004) and Caltech-256 (Griffin et al., 2007) image classification benchmarks. Caltech-101 contains 9K images labelled into 102 classes (101 object categories and a background class), while Caltech-256 is larger with 31K images and 257 classes. A standard evaluation protocol on these datasets is to generate several random splits into training and test data and report the average recognition performance across the splits, which is measured by the mean class recall (which compensates for a different number of test images per class). Following Chatfield et al. (2014); Zeiler & Fergus (2013); He et al. (2014), on Caltech-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class. On Caltech-256 we also generated 3 splits, each of which contains 60 training images per class (and the rest is used for testing). In each split, 20% of training images were used as a validation set for hyper-parameter selection.

> **Caltech-101 和 Caltech-256 的图像分类。** 在本节中，我们评估 Caltech-101（Fei-Fei et al.，2004）和 Caltech-256（Griffin et al.，2007）图像的非常深的特征分类基准。 Caltech-101 包含 9K 图像，分为 102 个类别（101 个对象类别和一个背景类别），而 Caltech-256 更大，有 31K 图像和 257 个类别。这些数据集的标准评估协议是生成几个随机分割成训练和测试数据，并报告分割的平均识别性能，这是通过平均类召回率来衡量的（它补偿了每个类的不同数量的测试图像）。继查特菲尔德等人（2014）之后，泽勒和弗格斯 (2013)，He等人 (2014)，在 Caltech-101 上，我们生成了 3 个随机拆分为训练和测试数据，因此每个拆分每个类别包含 30 个训练图像，每个类别最多包含 50 个测试图像。在 Caltech-256 上，我们还生成了 3 个拆分，每个拆分包含每个类 60 个训练图像（其余用于测试）。在每次拆分中，20% 的训练图像被用作超参数选择的验证集。

We found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over multiple scales, performs better than averaging or max-pooling. This can be explained by the fact that in Caltech images objects typically occupy the whole image, so multi-scale image features are semantically different (capturing the whole object vs. object parts), and stacking allows a classifier to exploit such scale-specific representations. We used three scales Q ∈ {256, 384, 512}. Our models are compared to each other and the state of the art in Table 11. As can be seen, the deeper 19-layer Net-E performs better than the 16-layer Net-D, and their combination further improves the performance. On Caltech-101, our representations are competitive with the approach of He et al. (2014), which, however, performs significantly worse than our nets on VOC-2007. On Caltech-256, our features outperform the state of the art (Chatfield et al., 2014) by a large margin (8.6%).

> 我们发现，与 VOC 不同，在加州理工学院的数据集上，在多个尺度上计算的描述符堆叠比平均或最大池表现更好。 这可以通过以下事实来解释：在加州理工学院图像中，对象通常占据整个图像，因此多尺度图像特征在语义上是不同的（捕获整个对象与对象部分），并且堆叠允许分类器利用这种尺度- 具体的表示。 我们使用了三个尺度 Q ∈ {256, 384, 512}。 我们的模型在表 11 中进行了相互比较和最先进的模型。可以看出，更深的 19 层 Net-E 的性能优于 16 层的 Net-D，它们的组合进一步提高了性能。 在 Caltech-101 上，我们的表示与 He 等人的方法具有竞争力。 （2014），然而，它在 VOC-2007 上的表现比我们的网络差得多。 在 Caltech-256 上，我们的功能大大优于现有技术（Chatfield 等人，2014 年）（8.6%）。

**Action Classification on VOC-2012**. We also evaluated our best-performing image representation (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classification task (Everingham et al., 2015), which consists in predicting an action class from a single image, given a bounding box of the person performing the action. The dataset contains 4.6K training images, labelled into 11 classes. Similarly to the VOC-2012 object classification task, the performance is measured using the mAP. We considered two training settings: (i) computing the ConvNet features on the whole image and ignoring the provided bounding box; (ii) computing the features on the whole image and on the provided bounding box, and stacking them to obtain the final representation. The results are compared to other approaches in Table 12.

> **对 VOC-2012 的行动分类**。 我们还在 PASCAL VOC-2012 动作分类任务 (Everingham et al., 2015) 上评估了我们表现最好的图像表示（Net-D 和 Net-E 特征的堆叠），其中包括预测来自 单个图像，给定执行动作的人的边界框。 该数据集包含 4.6K 训练图像，标记为 11 个类。 与 VOC-2012 对象分类任务类似，性能是使用 mAP 测量的。 我们考虑了两种训练设置：（i）在整个图像上计算 ConvNet 特征并忽略提供的边界框； (ii) 计算整个图像和提供的边界框上的特征，并将它们堆叠以获得最终表示。 结果与表 12 中的其他方法进行了比较。

Our representation achieves the state of art on the VOC action classification task even without using the provided bounding boxes, and the results are further improved when using both images and bounding boxes. Unlike other approaches, we did not incorporate any task-specific heuristics, but relied on the representation power of very deep convolutional features.

> 即使不使用提供的边界框，我们的表示在 VOC 动作分类任务上也达到了最先进的水平，并且在同时使用图像和边界框时，结果得到了进一步改进。 与其他方法不同，我们没有结合任何特定于任务的启发式方法，而是依赖于非常深的卷积特征的表示能力。

**Other Recognition Tasks.** Since the public release of our models, they have been actively used by the research community for a wide range of image recognition tasks, consistently outperforming more shallow representations. For instance, Girshick et al. (2014) achieve the state of the object detection results by replacing the ConvNet of Krizhevsky et al. (2012) with our 16-layer model. Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been observed in semantic segmentation (Long et al., 2014), image caption generation (Kiros et al., 2014; Karpathy & Fei-Fei, 2014), texture and material recognition (Cimpoi et al., 2014; Bell et al., 2014).

> **其他识别任务。**自从我们的模型公开发布以来，它们一直被研究界积极用于广泛的图像识别任务，始终优于更浅的表示。 例如，Girshick 等人 (2014) 使用我们的 16 层模型，通过替换 Krizhevsky 等人（2012）的 ConvNet 来实现目标检测结果的状态。Krizhevsky 等人 (2012)在更浅层架构上获得了类似的收益。 在语义分割 (Long et al., 2014)、图像字幕生成 (Kiros et al., 2014; Karpathy & Fei-Fei, 2014)、纹理和材料识别 (Cimpoi et al., 2014; Bell et al., 2014）。

![在这里插入图片描述](https://img-blog.csdnimg.cn/0f40c1e423344cc7ab93f9a682f40f28.png#pic_center)


# C PAPER REVISIONS
Here we present the list of major paper revisions, outlining the substantial changes for the convenience of the reader.

**v1** Initial version. Presents the experiments carried out before the ILSVRC submission.

**v2** Adds post-submission ILSVRC experiments with training set augmentation using scale jittering, which improves the performance.

**v3** Adds generalisation experiments (Appendix B) on PASCAL VOC and Caltech image classification datasets. The models used for these experiments are publicly available.

**v4** The paper is converted to ICLR-2015 submission format. Also adds experiments with multiple crops for classification.

**v6** Camera-ready ICLR-2015 conference paper. Adds a comparison of the net B with a shallow net and the results on PASCAL VOC action classification benchmark.
