@[toc]

# 「最小二乘法」的提出背景

最小二乘法通常归功于高斯（Carl Friedrich Gauss，1795），但最小二乘法是由阿德里安-马里·勒让德（Adrien-Marie Legendre）首先发表的。它对应的英文是 **least squares method**，在大陆地区的翻译一般是最小二乘法，或最小平方法。它是一种对离散数据求拟合，并通过均方差最小的约束条件来达成于目标数据之间最少误差的数学建模方法。

![在这里插入图片描述](https://img-blog.csdnimg.cn/13ae24def1eb4facb0f2986eba79f4a3.png#pic_center)

用上图进行解释会更明了一些，当我们有一组离散数据（图中红点所示），试图寻找某个能最大程度表示红点之间的趋势关系的函数 $f(x)$，这时就可以利用最小二乘法的概念，建模得到比较理想的解。

由于最小二乘法通常被表述为离散点对均值的回归问题，所以它被分类为一种 **回归问题**，而且也是回归问题中最常用的方法。对于最小二乘法来说，它的最佳拟合，即 **残差（残差为：观测值与模型提供的拟合值之间的差距）平方总和的最小化。** 当问题在自变量（x变量）有重大不确定性时，那么使用简易回归和最小二乘法会发生问题；在这种情况下，须另外考虑变量-误差-拟合模型所需的方法，而不是最小二乘法。

最小二乘问题分为两种：**线性或普通的最小二乘法**，和 **非线性的最小二乘法**，取决于在所有未知数中的残差是否为线性。线性的最小二乘问题发生在统计回归分析中；它有一个封闭形式的解决方案。非线性的问题通常经由迭代细致化来解决；在每次迭代中，系统由线性近似，因此在这两种情况下核心演算是相同的。

在本章中，我会介绍线性问题的求解，而对于非线性问题的求解，可以参考我其他博文的内容。

# 从一个简单的例子开始

假设某次实验得到4个离散数据 $(x, y)$，它们分别如下

x | y
--|---
1 | 6
2 | 5
3 | 7
4 | 10

现在我们要试图找到一条直线 $y = \omega x + b$，它与上述数据的距离最短，于是可以有

$$
S = [6 - (1\omega + b)] ^2 + [5 - (2 \omega + b)]^2 + \\
[7 - (3 \omega + b)]^2 + [10 - (4 \omega + b)]^2
$$

$$
S = 210 - 154 \omega - 56 b + 30 \omega^2 + 20 \omega b + 4 b^2
$$

这里的 $S$ 表示残差，或者说叫方差和，我们的目标是找到最合适的 $\omega$ 和 $b$，使得 $S$ 最小。找到最小组合的方法，除了一个参数一个参数的试验以外，还可以利用导数的性质，找到令偏导数 $\frac{\partial S}{\partial \omega} = 0$ 和 $\frac{\partial S}{\partial b} = 0$的情况，对于我们来说，求导数显然最为便捷，于是：

$$
\frac{\partial S}{\partial \omega} = -154 + 60 \omega + 20b = 0
$$

$$
\frac{\partial S}{\partial b} = -56 + 20 \omega + 8 b= 0
$$

于是最终得到

$$
\left \{ \begin{matrix}
\omega = 1.4 \\
b = 3.5
\end{matrix} \right.
$$

我们即得到方程

$$
y = 1.4 x + 3.5
$$

是对上述问题最佳的拟合函数。


# 参考资料

* https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95

